# Poker Bot - Advanced AI Poker Agent

A comprehensive poker bot system with multiple AI agents, including advanced CFR with pruning, DQN, and unified champion agents. The system features **a fully optimized DeepStack training pipeline**, vision-based game state detection, distributed training, and real-time search capabilities.

## ğŸš€ What's New

### Unified PokerBot Agent ğŸ¯ (Latest)

The pokerbot now features a single, unified world-class agent combining all best features:

- **âœ… Modular Architecture** - Configure exactly what you need
- **âœ… All Features Included** - CFR/CFR+, DQN, DeepStack, Opponent Modeling
- **âœ… Ensemble Decision Making** - Best-of-breed from multiple AI techniques
- **âœ… Comprehensive Tests** - Full test coverage and validation
- **âœ… Easy to Use** - Simple API with sensible defaults

**Quick Start:**
```bash
# Create and test the unified agent
python examples/test_pokerbot.py

# Train with the new agent
python scripts/train.py --agent-type pokerbot --mode smoketest
```

**Code Example:**
```python
from agents import create_agent

# Create agent with all features
agent = create_agent('pokerbot', name='MyBot')

# Or customize components
agent = create_agent('pokerbot', 
                    use_cfr=True, 
                    use_dqn=True,
                    use_deepstack=True,
                    cfr_weight=0.4,
                    dqn_weight=0.3,
                    deepstack_weight=0.3)
```

See [Migration Guide](docs/MIGRATION_GUIDE.md) for detailed documentation.

### Optimized DeepStack Training System âœ¨

The DeepStack neural network training pipeline has been completely optimized and is now production-ready:

- **âœ… Fixed all import issues** - Proper module structure and imports
- **âœ… Corrected training configuration** - Accurate num_buckets (36) matching training data
- **âœ… Enhanced training script** - Early stopping, LR scheduling, gradient clipping
- **âœ… Model validation tools** - Comprehensive quality assessment
- **âœ… Complete documentation** - Full guides and integration examples

**Quick Start:**
```bash
# Train DeepStack value network (5 min on GPU)
python scripts/train_deepstack.py --use-gpu

# Validate trained model
python scripts/validate_deepstack_model.py
```

See [DeepStack Training Guide](docs/DEEPSTACK_TRAINING.md) for complete documentation.

## Features

### ğŸ† Advanced AI Agents

- **PokerBot Agent (NEW!)**: Unified world-class agent combining all features âœ¨
  - Modular architecture with configurable components
  - CFR/CFR+ for game-theoretic optimal play
  - DQN for pattern recognition and learning
  - DeepStack continual re-solving
  - Opponent modeling and pre-trained models
  - **Recommended for all new development**
- **Champion Agent**: Unified CFR + DQN hybrid with pre-trained models (deprecated, use PokerBot)
- **Elite Unified Agent**: Advanced multi-component agent (deprecated, use PokerBot)
- **DeepStack Engine**: Complete Python/PyTorch port of the championship DeepStack AI
  - Continual re-solving for dynamic game tree solving
  - Neural network value estimation for depth-limited search
  - CFR-based game-theoretic optimal play
- **Search Agent**: Real-time depth-limited search with blueprint strategy
- **Advanced CFR Agent**: CFR with pruning, linear discounting, and progressive training
- **DQN Agent**: Deep reinforcement learning agent using Q-learning
- **CFR Agent**: Counterfactual Regret Minimization for game-theoretic optimal play
- **Fixed Strategy Agent**: GTO-inspired fixed strategy with pot odds calculations
- **Random Agent**: Baseline random decision-making agent

> ğŸ“– **Migration Guide**: See [docs/MIGRATION_GUIDE.md](docs/MIGRATION_GUIDE.md) for details on using the new PokerBot agent.

### ğŸš€ Progressive Training Pipeline

- **Multi-phase CFR Training**: Warmup â†’ Pruning â†’ Linear CFR â†’ Strategy updates
- **Distributed Training**: Multiprocessing for 4-8x speedup
- **Card Abstraction**: Information set clustering for memory efficiency
- **Vicarious Learning**: Multi-agent training and imitation learning

### ğŸ¯ Advanced Features

- **DeepStack Continual Re-solving**: Dynamic game tree solving during live play
  - Depth-limited search with neural network value estimation
  - CFRDGadget for opponent range reconstruction
  - Sub-second decision making via efficient CFR
- **Blueprint + Real-time Search**: Two-stage decision making
- **Information Set Abstraction**: Reduces 56B+ states to tractable sizes
- **CFR with Pruning (CFRp)**: 20x+ speedup through action pruning
- **Linear CFR**: Faster convergence via discounting
- **Pre-trained Models**: DeepStack value networks and equity tables

### ğŸ® Vision System

- Automated game state detection using GPT-4 Vision
- Screen capture and control
- Automatic action execution

### ğŸ“Š Training & Evaluation

- Comprehensive training framework
- Distributed training with multiprocessing
- Agent comparison and evaluation system
- Performance statistics and analytics

## Project Structure

```
pokerbot/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ agents/            # AI agent implementations
â”‚   â”‚   â”œâ”€â”€ champion_agent.py
â”‚   â”‚   â”œâ”€â”€ search_agent.py
â”‚   â”‚   â”œâ”€â”€ advanced_cfr.py
â”‚   â”‚   â”œâ”€â”€ cfr_agent.py
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ deepstack/         # DeepStack AI engine (ported from Lua)
â”‚   â”‚   â”œâ”€â”€ tree_builder.py      # Game tree construction
â”‚   â”‚   â”œâ”€â”€ tree_cfr.py          # CFR solver
â”‚   â”‚   â”œâ”€â”€ terminal_equity.py   # Equity calculation
â”‚   â”‚   â”œâ”€â”€ cfrd_gadget.py       # Range reconstruction
â”‚   â”‚   â”œâ”€â”€ value_nn.py          # Neural network
â”‚   â”‚   â”œâ”€â”€ resolving.py         # Continual re-solving API
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ game/              # Game engine and utilities
â”‚   â”‚   â”œâ”€â”€ card_abstraction.py
â”‚   â”‚   â”œâ”€â”€ game_state.py
â”‚   â”‚   â”œâ”€â”€ hand_evaluator.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ evaluation/        # Training and evaluation
â”‚   â”‚   â”œâ”€â”€ distributed_trainer.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ utils/             # Utility modules
â”œâ”€â”€ scripts/               # Executable scripts
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ play.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ tests/                 # Test suite
â”‚   â””â”€â”€ test_deepstack_core.py
â”œâ”€â”€ examples/              # Examples and demos
â”‚   â”œâ”€â”€ demo_champion.py
â”‚   â”œâ”€â”€ example_champion.py
â”‚   â””â”€â”€ test_champion.py
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ PORTING_BLUEPRINT.md
â”‚   â”œâ”€â”€ DEEPSTACK_GUIDE.md
â”‚   â”œâ”€â”€ CHAMPION_AGENT.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/                  # Training data and documentation
â”‚   â”œâ”€â”€ doc/              # Original DeepStack Lua documentation
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/                # Saved models and pre-trained weights
â””â”€â”€ README.md
```

## Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Setup

1. Clone the repository:
```bash
git clone https://github.com/elliotttmiller/pokerbot.git
cd pokerbot
```

2. Create a virtual environment:
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables (optional):
```bash
export OPENAI_API_KEY=your_api_key_here
```
```bash
pip install -r requirements.txt
```

4. Set up environment variables (optional):
```bash
export OPENAI_API_KEY=your_api_key_here
```

## Usage

### ğŸš€ Using DeepStack Engine

The DeepStack engine provides world-class game-theoretic poker play:

```python
from src.deepstack.resolving import Resolving
import numpy as np

# Initialize resolver
resolver = Resolving(num_hands=6, game_variant='leduc')

# Define game state
node_params = {
    'street': 0,
    'bets': [20, 20],
    'current_player': 1,
    'board': [],
    'bet_sizing': [1.0]  # Pot-sized bets
}

# Player and opponent ranges
player_range = np.ones(6) / 6
opponent_range = np.ones(6) / 6

# Solve using continual re-solving
resolver.resolve_first_node(node_params, player_range, opponent_range, 
                             iterations=500)

# Get optimal strategy
actions = resolver.get_possible_actions()
for action in actions:
    prob = resolver.get_action_strategy(action)
    print(f"P({action}) = {prob:.4f}")
```

**See [docs/DEEPSTACK_GUIDE.md](docs/DEEPSTACK_GUIDE.md) for complete DeepStack documentation.**

### ğŸ† Using the Champion Agent

The Champion Agent is our most advanced AI, combining CFR, DQN, and pre-trained models:

```bash
# Run the demo to see it in action
python demo_champion.py
```

Quick start in code:
```python
from src.agents import ChampionAgent
from src.game import Card, Rank, Suit

# Create champion agent with pre-trained knowledge
champion = ChampionAgent(name="Champion", use_pretrained=True)

# Make a decision
hole_cards = [Card(Rank.ACE, Suit.SPADES), Card(Rank.KING, Suit.HEARTS)]
action, raise_amount = champion.choose_action(
    hole_cards=hole_cards,
    community_cards=[],
    pot=100,
    current_bet=20,
    player_stack=1000,
    opponent_bet=20
)
```

**See [CHAMPION_AGENT.md](CHAMPION_AGENT.md) for complete documentation.**

### Training a DQN Agent

Train a new DQN agent through self-play:

```bash
python train.py --episodes 1000 --batch-size 32 --verbose
```

Options:
- `--episodes`: Number of training episodes (default: 1000)
- `--batch-size`: Batch size for training (default: 32)
- `--save-interval`: Save model every N episodes (default: 100)
- `--model-dir`: Directory to save models (default: models)
- `--verbose`: Print training progress

### Evaluating Agents

Compare different agents against each other:

```bash
python evaluate.py --num-hands 1000 --agents dqn fixed random --verbose
```

Options:
- `--num-hands`: Number of hands to play (default: 1000)
- `--model-path`: Path to trained DQN model
- `--agents`: Agents to evaluate (choices: dqn, fixed, random)
- `--verbose`: Print evaluation progress

### Playing Automated Poker

Run the bot to play on an online poker site:

```bash
python play.py --agent fixed --max-hands 20 --delay 2.0 --verbose
```

Options:
- `--agent`: Agent type (choices: dqn, fixed)
- `--model-path`: Path to trained DQN model (for DQN agent)
- `--max-hands`: Maximum hands to play (default: 20)
- `--delay`: Delay between actions in seconds (default: 2.0)
- `--verbose`: Print detailed information

**Note**: Vision-based detection requires an OpenAI API key for GPT-4 Vision.

## Project Structure

```
pokerbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/              # AI agent implementations
â”‚   â”‚   â”œâ”€â”€ base_agent.py    # Base agent interface
â”‚   â”‚   â”œâ”€â”€ dqn_agent.py     # Deep Q-Network agent
â”‚   â”‚   â”œâ”€â”€ fixed_strategy_agent.py  # Fixed strategy agent
â”‚   â”‚   â””â”€â”€ random_agent.py  # Random agent
â”‚   â”œâ”€â”€ game/                # Game engine
â”‚   â”‚   â”œâ”€â”€ card.py          # Card and deck classes
â”‚   â”‚   â”œâ”€â”€ hand_evaluator.py  # Hand ranking evaluation
â”‚   â”‚   â””â”€â”€ game_state.py    # Game state management
â”‚   â”œâ”€â”€ vision/              # Vision and automation
â”‚   â”‚   â”œâ”€â”€ vision_detector.py  # Game state detection
â”‚   â”‚   â””â”€â”€ screen_controller.py  # Screen control
â”‚   â”œâ”€â”€ evaluation/          # Training and evaluation
â”‚   â”‚   â”œâ”€â”€ trainer.py       # Training framework
â”‚   â”‚   â””â”€â”€ evaluator.py     # Evaluation framework
â”‚   â””â”€â”€ utils/               # Utilities
â”‚       â”œâ”€â”€ config.py        # Configuration management
â”‚       â””â”€â”€ logger.py        # Logging utilities
â”œâ”€â”€ data/                    # Data files
â”œâ”€â”€ models/                  # Trained models
â”œâ”€â”€ screenshots/             # Game screenshots
â”œâ”€â”€ logs/                    # Log files
â”œâ”€â”€ train.py                 # Training script
â”œâ”€â”€ evaluate.py              # Evaluation script
â”œâ”€â”€ play.py                  # Automated playing script
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md               # This file
```

## Architecture

### Game Engine

The game engine provides a complete implementation of Texas Hold'em poker:
- Card representation with ranks and suits
- Deck management with shuffling
- Hand evaluation using standard poker rankings
- Game state management with betting rounds
- Player actions (fold, check, call, raise)

### AI Agents

#### DQN Agent
- Uses deep neural networks to learn optimal strategy
- Experience replay for stable training
- Epsilon-greedy exploration strategy
- State encoding with normalized features

#### Fixed Strategy Agent
- Implements GTO-inspired strategy
- Pot odds calculations
- Hand strength evaluation
- Pre-flop and post-flop strategies

#### Random Agent
- Baseline for comparison
- Makes random valid decisions

### Vision System

- **VisionDetector**: Uses GPT-4 Vision to extract game state from screenshots
- **ScreenController**: Captures screenshots and controls mouse/keyboard
- **ActionMapper**: Maps poker actions to screen coordinates

## Configuration

Configuration can be set via environment variables:

```bash
# Game settings
export STARTING_STACK=1000
export SMALL_BLIND=10
export BIG_BLIND=20

# Training settings
export NUM_EPISODES=1000
export BATCH_SIZE=32

# Vision settings
export OPENAI_API_KEY=your_key_here
export SCREENSHOT_DIR=screenshots

# Model settings
export MODEL_DIR=models
export MODEL_PATH=models/model_final.h5

# Logging
export VERBOSE=true
export LOG_DIR=logs
```

## Performance

Example results after 1000 hands:

| Agent | Wins | Win Rate | Total Winnings |
|-------|------|----------|----------------|
| Fixed Strategy | 927 | 92.7% | +$18,540 |
| DQN Agent | 51 | 5.1% | -$9,270 |
| Random Agent | 6 | 0.6% | -$9,270 |
| Ties | 16 | 1.6% | - |

*Note: DQN performance improves significantly after training.*

## References

This project incorporates concepts and techniques from:

1. **DeepStack** - https://www.deepstack.ai/
   - Complete Python/PyTorch port of the championship DeepStack AI
   - Continual re-solving with depth-limited search
   - Original Lua documentation in `data/doc/`
   
2. **GTO Poker Bot** - https://github.com/arnenoori/gto-poker-bot
   - Vision-based game state detection
   - Automated playing framework

3. **Poker AI** - https://github.com/dickreuter/Poker
   - Advanced poker strategies
   - Pre-trained models

4. **DeepStack-Leduc** - https://github.com/lifrordi/DeepStack-Leduc
   - Deep counterfactual regret minimization
   - Reference implementation

5. **Libratus** - https://noambrown.github.io/papers/17-IJCAI-Libratus.pdf
   - Game theory optimal strategies
   - Abstraction techniques

## License

MIT License - See LICENSE file for details

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Disclaimer

This software is for educational and research purposes only. Please ensure compliance with all applicable laws and terms of service when using this software.
